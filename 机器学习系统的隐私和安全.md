深度学习系统很容易被对抗样本所欺骗,从而导致错误的分类。使用在线深度学习系统进行分类的用户不得不向服务器公开他们的数据,这会导致隐私泄露．更糟糕的
是,深度学习的广泛使用加剧了这些安全风险。
# 安全威胁
   深度学习模型在训练阶段、预测阶段的过程容易受到的威胁。
   基于整个深度学习过程,我们将  隐私  保护的对象分类为:１)训练数据集;２)模型结构、算法和模型参数;３)预测数据与结果．
   - 训练数据对于一个公司来说是至关重要的,也是非常有价值的,它的泄露意味着公司资产的损失
   - 经过训练的模型被认为是深度学习系统的核心竞争力．通常,训练模型包含３种类型的数据资产:１)模型,例如传统的机器学习和深度神经网络;２)超参数,设计了训练
     算法的结构如网络层数和神经元个数;３)参数,为多层神经网络中一层到另一层的计算系数．,经过训练的模型具有极其重要的商业和创新价值．一旦模型被复制、泄露或提取,
     模型所有者的利益将受到严重损害。
   - 在预测输入和预测结果方面,隐私来自于深度学习系统的使用者和提供者．恶意的服务提供者可能会保留用户的预测数据和结果,以便从中提取敏感信息,或者用于其
     他目的．另一方面,预测输入和结果可能会受到不法分子的攻击,他们可以利用这些数据来为自己创造利润。
# 攻击方法    
    模型提取攻击、模型逆向攻击、投毒攻击和对抗攻击。
    模型提取和逆向攻击针对的是隐私，前者主要窃取模型的信息，后者主要获得训练数据集的信息。
    投毒攻击和对抗攻击针对的是安全，前者主要在训练阶段投放恶意数据从而降低模型的分类准确率，后者主要在预测阶段制造对抗样本来欺骗模型。
    如果在训练数据集中出现恶意样本,我们称之为投毒攻击;如果在预测数据中出现恶意样本,我们称之为对抗攻击。
    
  ## 模型提取攻击(model extraction attack)
       新兴的，难度较大
       模型提取攻击发生在训练好的模型上,主要用于窃取模型参数及非法获取模型．它违反了训练模型的保密性．在新的业务机器学习即服务(machinelearningasaservice,MLaaS)设置中,
       模型本身托管在一个安全的云服务中,它允许用户通过基于云的预测API查询模型．模型所有者通过让用户为预测API付费来实现模型的业务价值,
       所以机器学习模型属于商业秘密．此外,一个模型的训练过程需要收集大量的数据集,也需要大量的时间和巨大的计算能力,所以一旦提取出模型并对其滥用,就会给模型拥有者带来巨大的经济损失．
       
       攻击者试图窃取模型的参数和超参数．目前主流方法通过构建精确模型或相似模型来实现模型的提取．精确模型是指攻击者试图重建原始模型,或从原始模型
       计算参数或超参数;而相似模型是攻击者构建的一个在预测性能上相近的替代模型．窃取精确模型会损害模型所有者的核心商业资产,并为攻击者获取
       价值,而窃取相似模型通常用于生成可迁移的对抗样本。攻击者以某种方式提取到原始模型、参数或结构,便可以利用这些知识来确定决策边界,从而生成相应的反例
       超参数在文中称为λ,用于平衡目标函数中的损失函数和正则化项。
       
  > 攻击者通过深度学习系统提供的API向模型发送大量的预测数据,然后接收模型返回的类标签和置信度系数,计算出模型的参数,最后还原原始模型．这种攻击可以破坏模型本身的隐私,损害
    模型所有者的利益,为攻击者创造商业价值,还可以帮助实现模型逆向攻击和对抗攻击.
         
  ## 模型逆向攻击(model inversion attack)
       在早期的认识中,训练数据集和训练模型之间只有一个信息流,即从数据集到模型．事实上,许多研究表明还存在一个逆向信息
       流,即从模型信息中恢复数据集信息,这称为模型逆向攻击．模型逆向攻击是指将训练数据集信息从模型中逆向提取出来．它主要包括成员推理攻(membershipinferenceattack,MIA)
       和属性推理攻击(propertyinferenceattack,PIA)．MIA 主要对数据集中是否出现特定记录进行推断,即判断隶属度,这是目前研究的热点．
       PIA 则主要获取数据集的如性别分布、年龄分布、收入分布、患病率等属性信息．模型逆向攻击窃取了训练数据集中成员的私有信息,也损害了数据集所有者的商业价值．
       发生这种情况有２个原因:①不充分的隐私保护,如信息泄露;②不安全的算法．为了加强对个人隐私的保护,欧盟于２０１８年颁布GDPR,它明确界定了个人资料的隐私,并对其进行严格保护．
       
     - 成员推理攻击
      给定实例x 和对在数据集D上训练的分类模型Ft 的黑盒访问权,当训练Ft 时,对手是否能够在D 中很有信心地推断实例x 是否包含在D 中。
      可以通过３种方法实现:
      1. 训练攻击模型；
        攻击模型是一个二元分类器,用来推断目标记录的信息．它将成员推理问题转化为分类问题,可用于白盒和黑盒攻击。．很多研究还引入了影子模型来训练攻击模型,影子模型主要用
        来模拟目标模型,并生成攻击模型所需的数据集．当然,对影子模型的训练也会增加攻击代价．
      2. 概率信息计算：
         该方法利用概率信息推断隶属度,无需攻击模型。这种方法需要一定的前提假设和辅助信息来获得可靠的概率向量或二元结果。
      3. 相似样本生成：
         该方法通过训练生成的模型(如生成对抗网络(GAN))生成训练记录,其生成的样本与目标训练数据集的样本相似。
     - 属性推理攻击
      对训练数据集的统计属性进行推理．推理的属性主要是一些统计信息,例如人口数据集中男女比例是否均衡、人口样本中是否存在少数民族样
      本、医疗数据集中患癌病人的比重等．
      
   > 攻击者通过向模型提供预测数据得到模型的置信度系数,破坏用户或数据集的隐私(例如恢复人脸识别系统中的人脸信息)．逆向攻击包括成员推理攻击(MIA)和
     属性推理攻击(PIA)．在MIA 中,攻击者可以推断训练数据集中是否包含特定 的记录．在PIA 中,攻击者可以推测训练数据集中是否存在一定的统计特
     征．最近的研究发现,在人口训练数据集中,某些阶层的人(如妇女和少数民族)的样本代表性不足,会影响最终模型的表现．模型逆向攻击表明,信息
     不仅可以从数据集流向模型和预测结果,还可以从模型和预测结果反向流向数据集．
         
  ## 投毒攻击(poisoning attack)
       投毒攻击主要是指在训练或再训练过程中,通过攻击训练数据集或算法来操纵机器学习模型的预测．由于在安全机器学习领域中,数据
       通常是非平稳的,其分布可能随时间而变化,因此一些模型不仅在训练过程中生成,而且在周期性再训练过程中随时间而变化．攻击训练数据集的方法主
       要包括污染源数据、向训练数据集中添加恶意样本、修改训练数据集中的部分标签、删除训练数据集中的一些原有样本等．攻击算法利用了不安全的特征
       选择方法或训练过程算法的弱点．投毒攻击会增加训练合适模型的难度．它还可以在生成的模型中为攻击者添加一个后门,攻击者可以使模型的预测偏
       向他想要的方向．
       
       投毒攻击本质上是在训练数据上寻求全局或局部分布的扰动。
       
       投毒攻击主要在２个方面影响了正常模型．１)直接改变分类器的决策边界,破坏分类器的正常使用,使其不能正确地对正常样本进行分类,破坏了模型的可用性．这主要
       是通过错误标记数据实现的．攻击者使用错误的标签提交数据记录,或恶意修改训练数据集中现有数据的标签．２)在分类器中创建后门．它能正确地对正
       常样本进行分类,但会导致对特定数据的分类错误．攻击者可以通过后门进行有针对性的攻击,破坏模型的完整性．这主要是通过加入特定的数据实现的．
       它们向数据集提交包含特定特征(如水印)和标签的数据,而在其他数据记录中很可能没有这样的特征．外,他们还可以直接攻击特征选择算法．
       
       相应地,   防御方法  主要是通过增强训练算法的鲁棒性和保护数据集的安全性来实现的．
       
  ## 对抗攻击(adversarial attack)
       最广泛，．对抗攻击是一种探索性攻击,它破坏了模型的可用性
       对抗攻击是指将对抗样例提交到训练好的模型中,从而使模型预测错误,它也被称为逃避攻击(evasionattack)．对抗样本是从原来正常
       的样本上添加了轻微的扰动,可以导致分类模型分类错误的样本．对抗样本另外一个特点是仅造成模型分类错误,人还是可以将它正确分类．同样,在语
       音和文本识别领域,对抗样本也未对原文进行令人察觉的修改．在恶意软件检测领域,恶意软件作者在其软件上添加一些特殊的语句可以逃避反病毒软件的检测．
    
       防御策略主要从对抗样本的生成和攻击的过程进行考虑,包括对抗训练、基于区域的分类、输入变化、梯度正则化、蒸馏、数据处理和训练防御网络
    
# 防御方法
   防御机制分为４类:差分隐私、同态加密、安全多方计算和次优选择．
   ## 隐私保护
       隐私保护可以分为４种技术:１)差分隐私(DPＧdifferentialprivacy);２)同态加密(HE homomorphicencryption);３)安全多方计算（SMC securemulti partycomputation);
      ４)次优选择(SCＧsuboptimalchoice)。
  - 差分隐私，最大限度地提高数据查询的准确性,同时尽可能减少从统计数据库查询时识别其记录的机会．它主要通过删除个体特征并保留统计特征的方式来保护用户隐私
  - 同态加密，同态加密是这样一种加密函数:对明文进行环上的加法和乘法运算,然后对其进行加密,和先对明文进行加密,再对密文进行相应的运算,可以得到等价的结果,即En(x) XOR En(y)＝En(x＋y)．
    在深度学习中,同态加密通常被用来保护用户的预测数据和结果。．一些工作也保留了训练模型的隐私。
    HE通常是在有数据泄漏风险中使用的．由于解密的高度复杂性,HE可以有效地保护敏感数据不被解密和窃取．应用HE的主要负面影响
    是效率的降低,即错误传输问题、对密文的操作时间较长、加密后数据量急剧增加等。
    
  - 安全多方计算，为了在没有可信第三方的情况下,保证约定函数的安全计算,这始于百万富翁的问题．它主要采用的技术包括多方计算、加密电路和不经意传输．在深度学习过程中,其应用场景是多
    个数据方希望使用多个服务器对其联合数据进行模型训练．它们要求任何数据方或服务器不能从该过程中的任何其他数据方了解训练数据．安全多方计
    算可以保护训练数据集和训练模型
    
  - 次优选择，．该方法易于实现,且具有较低的时间成本,但其效果尚未经过大规模实践的检验．例如,为防止盗窃模型参数,一些研究人员可能对模型参数进行四舍五入处理,将噪声添加到类概
    率,拒绝特征空间里的异常请求,返回第２或第３类的最大概率等
    
    
  差分隐私通常保护训练数据集,同态加密模型和预测数据,安全多方计算在训练过程中保护数据集和模型,次优选择主要针对训练模型。      
**所有这些方法都在一定程度上失去了一些准确性,从而换取隐私保护的改善。**

  ## 安全
    要保护系统中原本存在的合法数据(模型参数、数据集等),是隐私问题．
    受到攻击的分类模型在训练过程中不会接触到这些对抗样本,这些恶意数据原本不在学习模型中．要防范系统中原本不存在的、可能引起模型出错的恶意数据,就是安全问题．


# 机器学习概述
  ## 有监督的机器学习
  - 模型训练阶段
    将训练数据集作为输入，最后生成模型。生成经过调优的训练模型及相关参数。
    在运行训练算法之前,传统机器学习需要人工提取和选择特征,深度学习则委托训练算法自动识别可靠而有效的特征。
    深度神经网络受到生物神经系统的启发,由成千上万个神经元组成,用来传递信息．深度学习受益于人工神经网络,通常使用更多的层来提取和转换特征。
  - 模型预测（推理阶段）
    接受用户或攻击者的输入并提供预测结果。
    
  
